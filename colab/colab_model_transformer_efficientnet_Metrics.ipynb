{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWUxMo_dNy51",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "! wget http://images.cocodataset.org/zips/train2014.zip\n",
        "! wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
        "! sudo apt-get install unzip\n",
        "! unzip train2014.zip -d coco\n",
        "! unzip annotations_trainval2014.zip -d coco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "A7lcLoM9OMFI"
      },
      "outputs": [],
      "source": [
        "!pip install -r https://raw.githubusercontent.com/alexcalabrese/fdl_project/main/requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FydDSaeCHY9"
      },
      "source": [
        "# Image Captioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "69nYImuaNy59"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "class DatasetLoader:\n",
        "\n",
        "    def __init__(self, folder_name: str = \"coco\"):\n",
        "        self.data = self.load_data(folder_name)\n",
        "\n",
        "    def load_data(self, folder_name: str = \"coco\"):\n",
        "        print(\"Loading data..\")\n",
        "        # Set the data folder\n",
        "        data_folder = os.path.join(folder_name)\n",
        "\n",
        "        # Create the directory if it doesn't exist\n",
        "        if not os.path.exists(data_folder):\n",
        "            os.makedirs(data_folder)\n",
        "\n",
        "        # Path to the captions JSON file\n",
        "        filename = os.path.join(data_folder, \"annotations\", \"captions_train2014.json\")\n",
        "\n",
        "        # Read and parse the JSON file\n",
        "        with open(filename, \"r\") as json_file:\n",
        "            data = json.load(json_file)\n",
        "\n",
        "        print(\"Data loaded succesfully!\")\n",
        "\n",
        "        return data\n",
        "\n",
        "    def get_data_dict(self):\n",
        "        return self.data\n",
        "\n",
        "    def get_df_annotations(self):\n",
        "        return pd.DataFrame(self.data[\"annotations\"])\n",
        "\n",
        "    def get_df_images(self):\n",
        "        return pd.DataFrame(self.data[\"images\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLo0W6vdCHY_"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LvIF03YJCHZA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.applications import efficientnet\n",
        "from keras.layers import TextVectorization\n",
        "\n",
        "keras.utils.set_random_seed(111)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4kll22_CHZA"
      },
      "source": [
        "## Download the dataset\n",
        "\n",
        "We will be using the Flickr8K dataset for this tutorial. This dataset comprises over\n",
        "8,000 images, that are each paired with five different captions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Bo18px8lCHZB"
      },
      "outputs": [],
      "source": [
        "# Desired image dimensions\n",
        "IMAGE_SIZE = (299, 299)\n",
        "\n",
        "# Vocabulary size\n",
        "VOCAB_SIZE = 10000\n",
        "\n",
        "# Fixed length allowed for any sequence\n",
        "SEQ_LENGTH = 25\n",
        "\n",
        "# Dimension for the image embeddings and token embeddings\n",
        "EMBED_DIM = 512\n",
        "\n",
        "# Per-layer units in the feed-forward network\n",
        "FF_DIM = 512\n",
        "\n",
        "# Other training parameters\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 15\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsKZ446zCHZB"
      },
      "source": [
        "## Preparing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpUK3oFSNy6B",
        "outputId": "24e033f0-6291-4a17-fedd-fbb4e3edd115"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data..\n",
            "Data loaded succesfully!\n",
            "Loading data..\n",
            "Data loaded succesfully!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.xception import preprocess_input\n",
        "# from fdl_project.classes.dataset_loader import DatasetLoader\n",
        "\n",
        "# Load the dataset\n",
        "dataset = DatasetLoader().load_data(folder_name=os.path.join(os.getcwd(), \"coco\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Bm3LtvACHZB",
        "outputId": "098cc457-849e-47dc-9dcf-4e49d95e83c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples:  66226\n",
            "Number of validation samples:  16557\n"
          ]
        }
      ],
      "source": [
        "def load_captions_data(dataset):\n",
        "    text_annotations = []\n",
        "\n",
        "    # Create a mapping from image_id to file_name\n",
        "    image_id_to_file_name = {image[\"id\"]: image[\"file_name\"] for image in dataset[\"images\"]}\n",
        "\n",
        "    # Create a mapping from image_id to captions\n",
        "    image_id_to_captions = {}\n",
        "    for annotation in dataset[\"annotations\"]:\n",
        "        image_id = annotation[\"image_id\"]\n",
        "        caption = \"<start> \" + annotation[\"caption\"] + \" <end>\"\n",
        "\n",
        "        if image_id not in image_id_to_captions:\n",
        "            image_id_to_captions[image_id] = []\n",
        "\n",
        "        image_id_to_captions[image_id].append(caption)\n",
        "        text_annotations.append(caption)\n",
        "\n",
        "    # Create a combined dictionary with image file names and their corresponding captions\n",
        "    file_name_to_captions = {\n",
        "        image_id_to_file_name[image_id]: captions\n",
        "        for image_id, captions in image_id_to_captions.items()\n",
        "    }\n",
        "\n",
        "    return file_name_to_captions, text_annotations\n",
        "\n",
        "\n",
        "def train_val_split(caption_data, train_size=0.8, sample_size=0.001, shuffle=True):\n",
        "    all_images = list(caption_data.keys())\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.shuffle(all_images)\n",
        "\n",
        "    # Calculate the number of samples based on the sample_size percentage\n",
        "    num_samples = int(len(caption_data) * sample_size)\n",
        "\n",
        "    # Reduce the dataset to the specified sample size\n",
        "    all_images = all_images[:num_samples]\n",
        "\n",
        "    train_size = int(len(all_images) * train_size)\n",
        "\n",
        "    training_data = {\n",
        "        img_name: caption_data[img_name] for img_name in all_images[:train_size]\n",
        "    }\n",
        "    validation_data = {\n",
        "        img_name: caption_data[img_name] for img_name in all_images[train_size:]\n",
        "    }\n",
        "\n",
        "    return training_data, validation_data\n",
        "\n",
        "# Load the dataset\n",
        "captions_mapping, text_data = load_captions_data(dataset)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_data, valid_data = train_val_split(captions_mapping, sample_size=1.0)\n",
        "print(\"Number of training samples: \", len(train_data))\n",
        "print(\"Number of validation samples: \", len(valid_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "c0ZOttxpNy6D"
      },
      "outputs": [],
      "source": [
        "# captions_mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "727k9AQ_CHZC"
      },
      "source": [
        "## Vectorizing the text data\n",
        "\n",
        "We'll use the `TextVectorization` layer to vectorize the text data,\n",
        "that is to say, to turn the\n",
        "original strings into integer sequences where each integer represents the index of\n",
        "a word in a vocabulary. We will use a custom string standardization scheme\n",
        "(strip punctuation characters except `<` and `>`) and the default\n",
        "splitting scheme (split on whitespace)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8OqgwC3YCHZC"
      },
      "outputs": [],
      "source": [
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "\n",
        "strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
        "strip_chars = strip_chars.replace(\"<\", \"\")\n",
        "strip_chars = strip_chars.replace(\">\", \"\")\n",
        "\n",
        "vectorization = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=SEQ_LENGTH,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "vectorization.adapt(text_data)\n",
        "\n",
        "# Data augmentation for image data\n",
        "image_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.2),\n",
        "        layers.RandomContrast(0.3),\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3KIwKe9CHZC"
      },
      "source": [
        "## Building a `tf.data.Dataset` pipeline for training\n",
        "\n",
        "We will generate pairs of images and corresponding captions using a `tf.data.Dataset` object.\n",
        "The pipeline consists of two steps:\n",
        "\n",
        "1. Read the image from the disk\n",
        "2. Tokenize all the five captions corresponding to the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zy8wG46kNy6F"
      },
      "outputs": [],
      "source": [
        "# append coco/train2014/ to all the keys of the dictionary\n",
        "\n",
        "train_data = {\n",
        "    os.path.join(\"coco/train2014/\", key): value for key, value in train_data.items()\n",
        "}\n",
        "\n",
        "valid_data = {\n",
        "    os.path.join(\"coco/train2014/\", key): value for key, value in valid_data.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lSsWSC4oNy6F"
      },
      "outputs": [],
      "source": [
        "# train_data[\"coco/train2014/COCO_train2014_000000254362.jpg\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Mnstpb7cNy6F"
      },
      "outputs": [],
      "source": [
        "# check in train_data if there are more than 5 values\n",
        "# remove the 6th value and keep the first 5 values\n",
        "\n",
        "for key, value in train_data.items():\n",
        "    if len(value) > 5:\n",
        "        # print(key, len(value))\n",
        "        train_data[key] = value[:5]\n",
        "\n",
        "for key, value in valid_data.items():\n",
        "    if len(value) > 5:\n",
        "        # print(key, len(value))\n",
        "        valid_data[key] = value[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajmZktjnCHZC",
        "outputId": "a5d00bbf-38f0-4c8e-aacf-30663227a0ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROCESS_INPUT: LEN DATASET 4\n",
            "PROCESS_INPUT: LEN DATASET 4\n"
          ]
        }
      ],
      "source": [
        "def decode_and_resize(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SIZE)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    return img\n",
        "\n",
        "\n",
        "def process_input(img_path, captions):\n",
        "    print(\"PROCESS_INPUT: LEN DATASET\", len(dataset))\n",
        "    return decode_and_resize(img_path), vectorization(captions)\n",
        "\n",
        "\n",
        "def make_dataset(images, captions):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((images, tf.ragged.constant((captions))))\n",
        "    dataset = dataset.shuffle(BATCH_SIZE * 8)\n",
        "    dataset = dataset.map(process_input, num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Pass the list of images and the list of corresponding captions\n",
        "train_dataset = make_dataset(list(train_data.keys()), list(train_data.values()))\n",
        "\n",
        "valid_dataset = make_dataset(list(valid_data.keys()), list(valid_data.values()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LagTSBZGCHZC"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "Our image captioning architecture consists of three models:\n",
        "\n",
        "1. A CNN: used to extract the image features\n",
        "2. A TransformerEncoder: The extracted image features are then passed to a Transformer\n",
        "                    based encoder that generates a new representation of the inputs\n",
        "3. A TransformerDecoder: This model takes the encoder output and the text data\n",
        "                    (sequences) as inputs and tries to learn to generate the caption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJYaesH8CHZC",
        "outputId": "e5671ea4-87e2-4f25-ddf6-19df7ba19211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "def get_cnn_model():\n",
        "    base_model = efficientnet.EfficientNetB0(\n",
        "        input_shape=(*IMAGE_SIZE, 3),\n",
        "        include_top=False,\n",
        "        weights=\"imagenet\",\n",
        "    )\n",
        "    # We freeze our feature extractor\n",
        "    base_model.trainable = False\n",
        "    base_model_out = base_model.output\n",
        "    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)\n",
        "    cnn_model = keras.models.Model(base_model.input, base_model_out)\n",
        "    return cnn_model\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.0\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.dense_1 = layers.Dense(embed_dim, activation=\"relu\")\n",
        "\n",
        "    def call(self, inputs, training, mask=None):\n",
        "        inputs = self.layernorm_1(inputs)\n",
        "        inputs = self.dense_1(inputs)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=None,\n",
        "            training=training,\n",
        "        )\n",
        "        out_1 = self.layernorm_2(inputs + attention_output_1)\n",
        "        return out_1\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_tokens = embedded_tokens * self.embed_scale\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class TransformerDecoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ff_dim = ff_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
        "        )\n",
        "        self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")\n",
        "        self.ffn_layer_2 = layers.Dense(embed_dim)\n",
        "\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "\n",
        "        self.embedding = PositionalEmbedding(\n",
        "            embed_dim=EMBED_DIM,\n",
        "            sequence_length=SEQ_LENGTH,\n",
        "            vocab_size=VOCAB_SIZE,\n",
        "        )\n",
        "        self.out = layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n",
        "\n",
        "        self.dropout_1 = layers.Dropout(0.3)\n",
        "        self.dropout_2 = layers.Dropout(0.5)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, training, mask=None):\n",
        "        inputs = self.embedding(inputs)\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
        "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
        "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=combined_mask,\n",
        "            training=training,\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "            training=training,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        ffn_out = self.ffn_layer_1(out_2)\n",
        "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
        "        ffn_out = self.ffn_layer_2(ffn_out)\n",
        "\n",
        "        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)\n",
        "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
        "        preds = self.out(ffn_out)\n",
        "        return preds\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [\n",
        "                tf.expand_dims(batch_size, -1),\n",
        "                tf.constant([1, 1], dtype=tf.int32),\n",
        "            ],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "class ImageCaptioningModel(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        cnn_model,\n",
        "        encoder,\n",
        "        decoder,\n",
        "        num_captions_per_image=5,\n",
        "        image_aug=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n",
        "        self.num_captions_per_image = num_captions_per_image\n",
        "        self.image_aug = image_aug\n",
        "\n",
        "    def calculate_loss(self, y_true, y_pred, mask):\n",
        "        loss = self.loss(y_true, y_pred)\n",
        "        mask = tf.cast(mask, dtype=loss.dtype)\n",
        "        loss *= mask\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
        "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
        "        accuracy = tf.math.logical_and(mask, accuracy)\n",
        "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
        "        mask = tf.cast(mask, dtype=tf.float32)\n",
        "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
        "\n",
        "    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):\n",
        "        encoder_out = self.encoder(img_embed, training=training)\n",
        "        batch_seq_inp = batch_seq[:, :-1]\n",
        "        batch_seq_true = batch_seq[:, 1:]\n",
        "        mask = tf.math.not_equal(batch_seq_true, 0)\n",
        "        batch_seq_pred = self.decoder(\n",
        "            batch_seq_inp, encoder_out, training=training, mask=mask\n",
        "        )\n",
        "        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n",
        "        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n",
        "        return loss, acc\n",
        "\n",
        "    def train_step(self, batch_data):\n",
        "        batch_img, batch_seq = batch_data\n",
        "        batch_loss = 0\n",
        "        batch_acc = 0\n",
        "\n",
        "        if self.image_aug:\n",
        "            batch_img = self.image_aug(batch_img)\n",
        "\n",
        "        # 1. Get image embeddings\n",
        "        img_embed = self.cnn_model(batch_img)\n",
        "\n",
        "        # 2. Pass each of the five captions one by one to the decoder\n",
        "        # along with the encoder outputs and compute the loss as well as accuracy\n",
        "        # for each caption.\n",
        "        for i in range(self.num_captions_per_image):\n",
        "            with tf.GradientTape() as tape:\n",
        "                loss, acc = self._compute_caption_loss_and_acc(\n",
        "                    img_embed, batch_seq[:, i, :], training=True\n",
        "                )\n",
        "\n",
        "                # 3. Update loss and accuracy\n",
        "                batch_loss += loss\n",
        "                batch_acc += acc\n",
        "\n",
        "            # 4. Get the list of all the trainable weights\n",
        "            train_vars = (\n",
        "                self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "            )\n",
        "\n",
        "            # 5. Get the gradients\n",
        "            grads = tape.gradient(loss, train_vars)\n",
        "\n",
        "            # 6. Update the trainable weights\n",
        "            self.optimizer.apply_gradients(zip(grads, train_vars))\n",
        "\n",
        "        # 7. Update the trackers\n",
        "        batch_acc /= float(self.num_captions_per_image)\n",
        "        self.loss_tracker.update_state(batch_loss)\n",
        "        self.acc_tracker.update_state(batch_acc)\n",
        "\n",
        "        # 8. Return the loss and accuracy values\n",
        "        return {\n",
        "            \"loss\": self.loss_tracker.result(),\n",
        "            \"acc\": self.acc_tracker.result(),\n",
        "        }\n",
        "\n",
        "    def test_step(self, batch_data):\n",
        "        batch_img, batch_seq = batch_data\n",
        "        batch_loss = 0\n",
        "        batch_acc = 0\n",
        "\n",
        "        # 1. Get image embeddings\n",
        "        img_embed = self.cnn_model(batch_img)\n",
        "\n",
        "        # 2. Pass each of the five captions one by one to the decoder\n",
        "        # along with the encoder outputs and compute the loss as well as accuracy\n",
        "        # for each caption.\n",
        "        for i in range(self.num_captions_per_image):\n",
        "            loss, acc = self._compute_caption_loss_and_acc(\n",
        "                img_embed, batch_seq[:, i, :], training=False\n",
        "            )\n",
        "\n",
        "            # 3. Update batch loss and batch accuracy\n",
        "            batch_loss += loss\n",
        "            batch_acc += acc\n",
        "\n",
        "        batch_acc /= float(self.num_captions_per_image)\n",
        "\n",
        "        # 4. Update the trackers\n",
        "        self.loss_tracker.update_state(batch_loss)\n",
        "        self.acc_tracker.update_state(batch_acc)\n",
        "\n",
        "        # 5. Return the loss and accuracy values\n",
        "        return {\n",
        "            \"loss\": self.loss_tracker.result(),\n",
        "            \"acc\": self.acc_tracker.result(),\n",
        "        }\n",
        "\n",
        "    def predict(self, batch_data):\n",
        "        batch_img, batch_seq = batch_data\n",
        "        batch_pred = []\n",
        "\n",
        "        # Get image embeddings\n",
        "        img_embed = self.cnn_model(batch_img)\n",
        "\n",
        "        # Pass each of the five captions one by one to the decoder\n",
        "        # along with the encoder outputs and generate the predicted captions\n",
        "        for i in range(self.num_captions_per_image):\n",
        "            pred = self.decoder.predict(\n",
        "                [img_embed, batch_seq[:, i, :]], batch_size=len(batch_img)\n",
        "            )\n",
        "            batch_pred.append(pred)\n",
        "\n",
        "        # Return the predicted captions\n",
        "        return batch_pred\n",
        "\n",
        "    # Example usage:\n",
        "    # batch_data = (batch_img, batch_seq)\n",
        "    # predicted_captions = caption_model.predict(batch_data)\n",
        "        # called automatically.\n",
        "        return [self.loss_tracker, self.acc_tracker]\n",
        "\n",
        "\n",
        "cnn_model = get_cnn_model()\n",
        "encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=1)\n",
        "decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=2)\n",
        "caption_model = ImageCaptioningModel(\n",
        "    cnn_model=cnn_model,\n",
        "    encoder=encoder,\n",
        "    decoder=decoder,\n",
        "    image_aug=image_augmentation,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rxhYJ-XCHZD"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX2bXGgcNy6I",
        "outputId": "d65b99c5-4429-4d65-cdc4-551517566e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "mkdir: cannot create directory ‘/content/gdrive/My Drive/Colab_Models’: File exists\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    import datetime\n",
        "    drive.mount('/content/gdrive')\n",
        "\n",
        "    # Create a directory in Google Drive to store models\n",
        "    !mkdir /content/gdrive/My\\ Drive/Colab_Models\n",
        "    loss_model_path = \"/content/gdrive/My Drive/Colab_Models/loss_model_transformer_efficientnet_\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".txt\"\n",
        "    checkpoint_path = \"/content/gdrive/My Drive/Colab_Models/cp_model_transformer_efficientnet_\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".weights.h5\"\n",
        "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "    open(loss_model_path, 'w').close()\n",
        "except:\n",
        "    loss_model_path = \"loss_model_from_scratch_LSTM_CNN.txt\"\n",
        "    checkpoint_path = \"cp_model_from_scratch_LSTM_CNN.weights.h5\"\n",
        "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "    open(loss_model_path, 'w').close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "lO59QfwcCHZD"
      },
      "outputs": [],
      "source": [
        "# Define the loss function\n",
        "cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=False,\n",
        "    reduction='none',\n",
        ")\n",
        "\n",
        "# EarlyStopping criteria\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "# Learning Rate Scheduler for the optimizer\n",
        "class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, post_warmup_learning_rate, warmup_steps):\n",
        "        super().__init__()\n",
        "        self.post_warmup_learning_rate = post_warmup_learning_rate\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        global_step = tf.cast(step, tf.float32)\n",
        "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
        "        warmup_progress = global_step / warmup_steps\n",
        "        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n",
        "        return tf.cond(\n",
        "            global_step < warmup_steps,\n",
        "            lambda: warmup_learning_rate,\n",
        "            lambda: self.post_warmup_learning_rate,\n",
        "        )\n",
        "\n",
        "\n",
        "# Create a learning rate schedule\n",
        "num_train_steps = len(train_dataset) * EPOCHS\n",
        "num_warmup_steps = num_train_steps // 15\n",
        "lr_schedule = LRSchedule(post_warmup_learning_rate=1e-4, warmup_steps=num_warmup_steps)\n",
        "\n",
        "# Compile the model\n",
        "caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRErB2zUNy6I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6487a90-1663-4f7a-9704-7c2284e66200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/Colab_Models/test_training_logs.json\n",
            "Epoch 1/15\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - acc: 0.2347 - loss: 28.3155\n",
            "Epoch 1: saving model to /content/gdrive/My Drive/Colab_Models/cp_model_transformer_efficientnet_20240829_105032.weights.h5\n",
            "Epoch 1 - BLEU1 Score: 0.15964056551456451, BLEU4 Score: 0.03013080544769764\n",
            "Epoch 1 - ROUGE-L Precision: 0.35711169242858887, Recall: 0.2678021788597107, F1 Score: 0.3028911352157593\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1318s\u001b[0m 5s/step - acc: 0.2351 - loss: 28.2891 - val_acc: 0.4232 - val_loss: 15.4409 - BLEU-1: 0.1596 - BLEU-4: 0.0301 - perplexity: 8688.7660 - ROUGE-L Precision: 0.3571 - ROUGE-L Recall: 0.2678 - ROUGE-L F1 Score: 0.3029 - epoch: 1.0000\n",
            "Epoch 2/15\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - acc: 0.4152 - loss: 15.6116\n",
            "Epoch 2: saving model to /content/gdrive/My Drive/Colab_Models/cp_model_transformer_efficientnet_20240829_105032.weights.h5\n",
            "Epoch 2 - BLEU1 Score: 0.1728970855474472, BLEU4 Score: 0.03608785197138786\n",
            "Epoch 2 - ROUGE-L Precision: 0.34746307134628296, Recall: 0.26119446754455566, F1 Score: 0.2948760986328125\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1248s\u001b[0m 5s/step - acc: 0.4153 - loss: 15.6101 - val_acc: 0.4526 - val_loss: 13.8755 - BLEU-1: 0.1729 - BLEU-4: 0.0361 - perplexity: 8511.7289 - ROUGE-L Precision: 0.3475 - ROUGE-L Recall: 0.2612 - ROUGE-L F1 Score: 0.2949 - epoch: 2.0000\n",
            "Epoch 3/15\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - acc: 0.4393 - loss: 14.3185\n",
            "Epoch 3: saving model to /content/gdrive/My Drive/Colab_Models/cp_model_transformer_efficientnet_20240829_105032.weights.h5\n",
            "Epoch 3 - BLEU1 Score: 0.1772959679365158, BLEU4 Score: 0.034510474652051926\n",
            "Epoch 3 - ROUGE-L Precision: 0.35542577505111694, Recall: 0.26812925934791565, F1 Score: 0.3020961880683899\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1309s\u001b[0m 5s/step - acc: 0.4394 - loss: 14.3179 - val_acc: 0.4675 - val_loss: 13.1684 - BLEU-1: 0.1773 - BLEU-4: 0.0345 - perplexity: 8515.1418 - ROUGE-L Precision: 0.3554 - ROUGE-L Recall: 0.2681 - ROUGE-L F1 Score: 0.3021 - epoch: 3.0000\n",
            "Epoch 4/15\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - acc: 0.4515 - loss: 13.6876\n",
            "Epoch 4: saving model to /content/gdrive/My Drive/Colab_Models/cp_model_transformer_efficientnet_20240829_105032.weights.h5\n",
            "Epoch 4 - BLEU1 Score: 0.18139240145683289, BLEU4 Score: 0.037944160401821136\n",
            "Epoch 4 - ROUGE-L Precision: 0.3558243215084076, Recall: 0.2715446949005127, F1 Score: 0.30444249510765076\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1281s\u001b[0m 5s/step - acc: 0.4515 - loss: 13.6872 - val_acc: 0.4754 - val_loss: 12.7997 - BLEU-1: 0.1814 - BLEU-4: 0.0379 - perplexity: 8449.0809 - ROUGE-L Precision: 0.3558 - ROUGE-L Recall: 0.2715 - ROUGE-L F1 Score: 0.3044 - epoch: 4.0000\n",
            "Epoch 5/15\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - acc: 0.4593 - loss: 13.2761\n",
            "Epoch 5: saving model to /content/gdrive/My Drive/Colab_Models/cp_model_transformer_efficientnet_20240829_105032.weights.h5\n",
            "Epoch 5 - BLEU1 Score: 0.18375174701213837, BLEU4 Score: 0.03829585015773773\n",
            "Epoch 5 - ROUGE-L Precision: 0.35368549823760986, Recall: 0.2710365056991577, F1 Score: 0.30337822437286377\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1267s\u001b[0m 5s/step - acc: 0.4593 - loss: 13.2758 - val_acc: 0.4803 - val_loss: 12.5607 - BLEU-1: 0.1838 - BLEU-4: 0.0383 - perplexity: 8410.6256 - ROUGE-L Precision: 0.3537 - ROUGE-L Recall: 0.2710 - ROUGE-L F1 Score: 0.3034 - epoch: 5.0000\n",
            "Epoch 6/15\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - acc: 0.4652 - loss: 12.9852\n",
            "Epoch 6: saving model to /content/gdrive/My Drive/Colab_Models/cp_model_transformer_efficientnet_20240829_105032.weights.h5\n",
            "Epoch 6 - BLEU1 Score: 0.18552932143211365, BLEU4 Score: 0.04029546305537224\n",
            "Epoch 6 - ROUGE-L Precision: 0.35471439361572266, Recall: 0.27182191610336304, F1 Score: 0.3043898642063141\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1279s\u001b[0m 5s/step - acc: 0.4652 - loss: 12.9849 - val_acc: 0.4839 - val_loss: 12.3914 - BLEU-1: 0.1855 - BLEU-4: 0.0403 - perplexity: 8410.1607 - ROUGE-L Precision: 0.3547 - ROUGE-L Recall: 0.2718 - ROUGE-L F1 Score: 0.3044 - epoch: 6.0000\n",
            "Epoch 7/15\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - acc: 0.4703 - loss: 12.7463\n",
            "Epoch 7: saving model to /content/gdrive/My Drive/Colab_Models/cp_model_transformer_efficientnet_20240829_105032.weights.h5\n",
            "Epoch 7 - BLEU1 Score: 0.18541039526462555, BLEU4 Score: 0.040532082319259644\n",
            "Epoch 7 - ROUGE-L Precision: 0.36021414399147034, Recall: 0.2755599319934845, F1 Score: 0.3088284432888031\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1270s\u001b[0m 5s/step - acc: 0.4703 - loss: 12.7461 - val_acc: 0.4869 - val_loss: 12.2683 - BLEU-1: 0.1854 - BLEU-4: 0.0405 - perplexity: 8396.2044 - ROUGE-L Precision: 0.3602 - ROUGE-L Recall: 0.2756 - ROUGE-L F1 Score: 0.3088 - epoch: 7.0000\n",
            "Epoch 8/15\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - acc: 0.4742 - loss: 12.5529\n",
            "Epoch 8: saving model to /content/gdrive/My Drive/Colab_Models/cp_model_transformer_efficientnet_20240829_105032.weights.h5\n",
            "Epoch 8 - BLEU1 Score: 0.18743227422237396, BLEU4 Score: 0.042701493948698044\n",
            "Epoch 8 - ROUGE-L Precision: 0.3642209470272064, Recall: 0.27831169962882996, F1 Score: 0.3120938539505005\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1289s\u001b[0m 5s/step - acc: 0.4742 - loss: 12.5528 - val_acc: 0.4892 - val_loss: 12.1705 - BLEU-1: 0.1874 - BLEU-4: 0.0427 - perplexity: 8380.4774 - ROUGE-L Precision: 0.3642 - ROUGE-L Recall: 0.2783 - ROUGE-L F1 Score: 0.3121 - epoch: 8.0000\n",
            "Epoch 9/15\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - acc: 0.4774 - loss: 12.3980\n",
            "Epoch 9: saving model to /content/gdrive/My Drive/Colab_Models/cp_model_transformer_efficientnet_20240829_105032.weights.h5\n",
            "Epoch 9 - BLEU1 Score: 0.18905143439769745, BLEU4 Score: 0.04290349781513214\n",
            "Epoch 9 - ROUGE-L Precision: 0.36435389518737793, Recall: 0.27825140953063965, F1 Score: 0.3121400773525238\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1267s\u001b[0m 5s/step - acc: 0.4774 - loss: 12.3979 - val_acc: 0.4907 - val_loss: 12.0927 - BLEU-1: 0.1891 - BLEU-4: 0.0429 - perplexity: 8335.4797 - ROUGE-L Precision: 0.3644 - ROUGE-L Recall: 0.2783 - ROUGE-L F1 Score: 0.3121 - epoch: 9.0000\n",
            "Epoch 10/15\n",
            "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - acc: 0.4805 - loss: 12.2525\n",
            "Epoch 10: saving model to /content/gdrive/My Drive/Colab_Models/cp_model_transformer_efficientnet_20240829_105032.weights.h5\n",
            "Epoch 10 - BLEU1 Score: 0.18936161696910858, BLEU4 Score: 0.04378179460763931\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import _pickle as pickle\n",
        "from keras_nlp.metrics import Bleu, Perplexity, RougeL\n",
        "from keras.callbacks import Callback\n",
        "import warnings\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import os\n",
        "\n",
        "# # Set TensorFlow logging level to ERROR to suppress warnings and info messages\n",
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LossHistory(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.history = {'loss':[],'val_loss':[]}\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.history['loss'].append(logs.get('loss'))\n",
        "        with open(loss_model_path, \"r+b\") as myfile:\n",
        "            myfile.write(pickle.dumps(self.history))\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.history['val_loss'].append(logs.get('val_loss'))\n",
        "\n",
        "\n",
        "\n",
        "class BleuScoreCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, validation_dataset, dim_batch, vectorization, index_lookup, max_decoded_sentence_length, n_batch=5):\n",
        "        super().__init__()\n",
        "        self.validation_dataset = validation_dataset\n",
        "        self.dim_batch = dim_batch\n",
        "        self.n_batch = n_batch\n",
        "        self.vectorization = vectorization\n",
        "        self.index_lookup = index_lookup\n",
        "        self.max_decoded_sentence_length = max_decoded_sentence_length\n",
        "        self.bleu_metric4 = Bleu( max_order=4)\n",
        "        self.bleu_metric3 = Bleu( max_order=3)\n",
        "        self.bleu_metric2 = Bleu( max_order=2)\n",
        "        self.bleu_metric1 = Bleu( max_order=1)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        references = []\n",
        "        predictions = []\n",
        "\n",
        "        for batch_data in self.validation_dataset.take(self.n_batch):\n",
        "            # reduce batch size if the number of samples is more than the dim_batch ( for limited memory devices)\n",
        "            if len(batch_data[0]) > self.dim_batch:\n",
        "                batch_data = (batch_data[0][:self.dim_batch], batch_data[1][:self.dim_batch])\n",
        "            batch_img, batch_seq = batch_data\n",
        "            img_embed = self.model.cnn_model(batch_img, training=False)\n",
        "            encoded_img = self.model.encoder(img_embed, training=False)\n",
        "\n",
        "            for i in range(batch_img.shape[0]):\n",
        "                decoded_caption = \"<start>\"\n",
        "                for j in range(self.max_decoded_sentence_length):\n",
        "                    tokenized_caption = self.vectorization([decoded_caption])[:, :-1]\n",
        "                    mask = tf.math.not_equal(tokenized_caption, 0)\n",
        "                    predictions_batch = self.model.decoder(\n",
        "                        tokenized_caption, encoded_img[i:i+1], training=False, mask=mask\n",
        "                    )\n",
        "\n",
        "                    if j < predictions_batch.shape[1]:\n",
        "                        sampled_token_index = np.argmax(predictions_batch[0, j, :])\n",
        "                        sampled_token = self.index_lookup[sampled_token_index]\n",
        "                        if sampled_token == \"<end>\":\n",
        "                            break\n",
        "                        decoded_caption += \" \" + sampled_token\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                decoded_caption = decoded_caption.replace(\"<start> \", \"\").replace(\" <end>\", \"\").strip()\n",
        "                pred_texts = decoded_caption\n",
        "\n",
        "                # Convert ground truth token IDs to text ## TO fix that in this way the 5 caption are concatenated, we need to split them get the bleu score separately\n",
        "                gt_texts = ' '.join([self.index_lookup[int(token)] for token in batch_seq[i, 1:].numpy().flatten() if int(token) != 0])\n",
        "\n",
        "\n",
        "                references.append([gt_texts])\n",
        "                predictions.append(pred_texts)\n",
        "\n",
        "        bleu_score = self.bleu_metric1(references, predictions).numpy()\n",
        "        print(f\"Epoch {epoch+1} - BLEU1 Score: {bleu_score}\")\n",
        "        logs['BLEU-4'] = self.bleu_metric4(references, predictions).numpy()\n",
        "        logs['BLEU-1'] = self.bleu_metric1(references, predictions).numpy()\n",
        "\n",
        "class BleuScoreCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, validation_dataset, dim_batch, vectorization, index_lookup, max_decoded_sentence_length, n_batch=5):\n",
        "        super().__init__()\n",
        "        self.validation_dataset = validation_dataset\n",
        "        self.dim_batch = dim_batch\n",
        "        self.n_batch = n_batch\n",
        "        self.vectorization = vectorization\n",
        "        self.index_lookup = index_lookup\n",
        "        self.max_decoded_sentence_length = max_decoded_sentence_length\n",
        "        self.bleu_metric4 = Bleu(max_order=4)\n",
        "        self.bleu_metric3 = Bleu(max_order=3)\n",
        "        self.bleu_metric2 = Bleu(max_order=2)\n",
        "        self.bleu_metric1 = Bleu(max_order=1)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        references = []\n",
        "        predictions = []\n",
        "\n",
        "        for batch_data in self.validation_dataset.take(self.n_batch):\n",
        "            # Reduce batch size if the number of samples is more than the dim_batch (for limited memory devices)\n",
        "            if len(batch_data[0]) > self.dim_batch:\n",
        "                batch_data = (batch_data[0][:self.dim_batch], batch_data[1][:self.dim_batch])\n",
        "            batch_img, batch_seq = batch_data\n",
        "            img_embed = self.model.cnn_model(batch_img, training=False)\n",
        "            encoded_img = self.model.encoder(img_embed, training=False)\n",
        "\n",
        "            for i in range(batch_img.shape[0]):\n",
        "                decoded_caption = \"<start>\"\n",
        "                for j in range(self.max_decoded_sentence_length):\n",
        "                    tokenized_caption = self.vectorization([decoded_caption])[:, :-1]\n",
        "                    mask = tf.math.not_equal(tokenized_caption, 0)\n",
        "                    predictions_batch = self.model.decoder(\n",
        "                        tokenized_caption, encoded_img[i:i+1], training=False, mask=mask\n",
        "                    )\n",
        "\n",
        "                    if j < predictions_batch.shape[1]:\n",
        "                        sampled_token_index = np.argmax(predictions_batch[0, j, :])\n",
        "                        sampled_token = self.index_lookup[sampled_token_index]\n",
        "                        if sampled_token == \"<end>\":\n",
        "                            break\n",
        "                        decoded_caption += \" \" + sampled_token\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                decoded_caption = decoded_caption.replace(\"<start> \", \"\").replace(\" <end>\", \"\").strip()\n",
        "                pred_texts = decoded_caption\n",
        "\n",
        "                # Convert ground truth token IDs to text and split them into individual captions\n",
        "                gt_texts = [\n",
        "                    ' '.join([self.index_lookup[int(token)] for token in caption if int(token) != 0])\n",
        "                    for caption in batch_seq[i].numpy()\n",
        "                ]\n",
        "\n",
        "                references.extend([[gt_text] for gt_text in gt_texts])\n",
        "                predictions.extend([pred_texts] * len(gt_texts))\n",
        "\n",
        "        bleu_score1 = self.bleu_metric1(references, predictions).numpy()\n",
        "        bleu_score4 = self.bleu_metric4(references, predictions).numpy()\n",
        "        print(f\"Epoch {epoch+1} - BLEU1 Score: {bleu_score1}, BLEU4 Score: {bleu_score4}\")\n",
        "        logs['BLEU-1'] = bleu_score1\n",
        "        logs['BLEU-4'] = bleu_score4\n",
        "\n",
        "class PerplexityCallback(Callback):\n",
        "    def __init__(self, validation_dataset, dim_batch, vectorization, index_lookup, max_decoded_sentence_length, n_batch=5, verbose=False):\n",
        "        super().__init__()\n",
        "        self.validation_dataset = validation_dataset\n",
        "        self.dim_batch = dim_batch\n",
        "        self.n_batch = n_batch\n",
        "        self.vectorization = vectorization\n",
        "        self.index_lookup = index_lookup\n",
        "        self.max_decoded_sentence_length = max_decoded_sentence_length\n",
        "        self.perplexity_metric = Perplexity(from_logits=True)\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def log(self, message):\n",
        "        if self.verbose:\n",
        "            print(message)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        total_perplexity = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_data in self.validation_dataset.take(self.n_batch):\n",
        "            if len(batch_data[0]) > self.dim_batch:\n",
        "                batch_data = (batch_data[0][:self.dim_batch], batch_data[1][:self.dim_batch])\n",
        "            batch_img, batch_seq = batch_data\n",
        "\n",
        "            self.log(f\"Batch sequence shape: {batch_seq.shape}\")\n",
        "\n",
        "            img_embed = self.model.cnn_model(batch_img, training=False)\n",
        "            encoded_img = self.model.encoder(img_embed, training=False)\n",
        "\n",
        "            for i in range(batch_img.shape[0]):  # For each image in the batch\n",
        "                image_perplexities = []\n",
        "                for c in range(batch_seq.shape[1]):  # For each caption of the image\n",
        "                    tokenized_caption = self.vectorization([\"<start>\"])[:, :1]  # Take only the <start> token\n",
        "                    predictions = []\n",
        "\n",
        "                    self.log(f\"Initial tokenized_caption shape: {tokenized_caption.shape}\")\n",
        "\n",
        "                    for j in range(1, min(self.max_decoded_sentence_length, batch_seq.shape[2] - 1)):\n",
        "                        try:\n",
        "                            mask = tf.math.not_equal(tokenized_caption, 0)\n",
        "                            pred_batch = self.model.decoder(\n",
        "                                tokenized_caption, encoded_img[i:i+1], training=False, mask=mask\n",
        "                            )\n",
        "                            predictions.append(pred_batch[:, -1:, :])  # Only keep the last prediction\n",
        "\n",
        "                            next_token = tf.reshape(batch_seq[i, c, j], (1, 1))\n",
        "                            self.log(f\"Next token shape: {next_token.shape}, value: {next_token.numpy()}\")\n",
        "                            tokenized_caption = tf.concat([tokenized_caption, next_token], axis=1)\n",
        "                            self.log(f\"Updated tokenized_caption shape: {tokenized_caption.shape}\")\n",
        "                        except tf.errors.InvalidArgumentError as e:\n",
        "                            self.log(f\"Error at position {j}. Skipping this token.\")\n",
        "                            continue\n",
        "\n",
        "                    if not predictions:\n",
        "                        self.log(\"No valid predictions were made. Skipping this caption.\")\n",
        "                        continue\n",
        "\n",
        "                    predictions = tf.concat(predictions, axis=1)\n",
        "                    target = batch_seq[i:i+1, c, 1:j]  # Remove <start> token and limit to predicted length\n",
        "\n",
        "                    self.log(f\"Predictions shape: {predictions.shape}\")\n",
        "                    self.log(f\"Target shape: {target.shape}\")\n",
        "\n",
        "                    # Ensure predictions and target have the same sequence length\n",
        "                    min_len = min(predictions.shape[1], target.shape[1])\n",
        "                    predictions = predictions[:, :min_len, :]\n",
        "                    target = target[:, :min_len]\n",
        "\n",
        "                    if min_len == 0:\n",
        "                        self.log(\"No valid tokens to calculate perplexity. Skipping this caption.\")\n",
        "                        continue\n",
        "\n",
        "                    try:\n",
        "                        self.perplexity_metric.update_state(target, predictions)\n",
        "                        caption_perplexity = self.perplexity_metric.result().numpy()\n",
        "                        image_perplexities.append(caption_perplexity)\n",
        "                    except tf.errors.InvalidArgumentError as e:\n",
        "                        self.log(f\"Error calculating perplexity: {e}\")\n",
        "                    finally:\n",
        "                        self.perplexity_metric.reset_state()\n",
        "\n",
        "                # Average perplexity across all captions for this image\n",
        "                if image_perplexities:\n",
        "                    avg_image_perplexity = sum(image_perplexities) / len(image_perplexities)\n",
        "                    total_perplexity += avg_image_perplexity\n",
        "                    num_batches += 1\n",
        "                else:\n",
        "                    self.log(\"No valid perplexities for this image. Skipping.\")\n",
        "\n",
        "        avg_perplexity = total_perplexity / num_batches if num_batches > 0 else 0\n",
        "        self.log(f\"Epoch {epoch+1} - Average Perplexity: {avg_perplexity:.4f}\")\n",
        "        logs['perplexity'] = avg_perplexity\n",
        "\n",
        "\n",
        "class RougeScoreCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, validation_dataset, dim_batch, vectorization, index_lookup, max_decoded_sentence_length, n_batch=5, verbose=False):\n",
        "        super().__init__()\n",
        "        self.validation_dataset = validation_dataset\n",
        "        self.dim_batch = dim_batch\n",
        "        self.n_batch = n_batch\n",
        "        self.vectorization = vectorization\n",
        "        self.index_lookup = index_lookup\n",
        "        self.max_decoded_sentence_length = max_decoded_sentence_length\n",
        "        self.rouge_metric = RougeL()\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def log(self, message):\n",
        "        if self.verbose:\n",
        "            print(message)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        references = []\n",
        "        predictions = []\n",
        "\n",
        "        for batch_data in self.validation_dataset.take(self.n_batch):\n",
        "            # Reduce batch size if the number of samples is more than the dim_batch (for limited memory devices)\n",
        "            if len(batch_data[0]) > self.dim_batch:\n",
        "                batch_data = (batch_data[0][:self.dim_batch], batch_data[1][:self.dim_batch])\n",
        "            batch_img, batch_seq = batch_data\n",
        "            img_embed = self.model.cnn_model(batch_img, training=False)\n",
        "            encoded_img = self.model.encoder(img_embed, training=False)\n",
        "\n",
        "            for i in range(batch_img.shape[0]):\n",
        "                decoded_caption = \"<start>\"\n",
        "                for j in range(self.max_decoded_sentence_length):\n",
        "                    tokenized_caption = self.vectorization([decoded_caption])[:, :-1]\n",
        "                    mask = tf.math.not_equal(tokenized_caption, 0)\n",
        "                    predictions_batch = self.model.decoder(\n",
        "                        tokenized_caption, encoded_img[i:i+1], training=False, mask=mask\n",
        "                    )\n",
        "\n",
        "                    if j < predictions_batch.shape[1]:\n",
        "                        sampled_token_index = np.argmax(predictions_batch[0, j, :])\n",
        "                        sampled_token = self.index_lookup[sampled_token_index]\n",
        "                        if sampled_token == \"<end>\":\n",
        "                            break\n",
        "                        decoded_caption += \" \" + sampled_token\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                decoded_caption = decoded_caption.replace(\"<start> \", \"\").replace(\" <end>\", \"\").strip()\n",
        "                pred_texts = decoded_caption\n",
        "\n",
        "                # Convert ground truth token IDs to text and split them into individual captions\n",
        "                gt_texts = [\n",
        "                    ' '.join([self.index_lookup[int(token)] for token in caption if int(token) != 0])\n",
        "                    for caption in batch_seq[i].numpy()\n",
        "                ]\n",
        "\n",
        "                references.extend(gt_texts)\n",
        "                predictions.extend([pred_texts] * len(gt_texts))\n",
        "\n",
        "                if self.verbose:\n",
        "                    for ref, pred in zip(gt_texts, [pred_texts] * len(gt_texts)):\n",
        "                        print(f\"Reference: {ref}\")\n",
        "                        print(f\"Prediction: {pred}\")\n",
        "\n",
        "        rouge_scores = [self.rouge_metric(ref, pred) for ref, pred in zip(references, predictions)]\n",
        "        rouge_precisions = [score['precision'].numpy() for score in rouge_scores]\n",
        "        rouge_recalls = [score['recall'].numpy() for score in rouge_scores]\n",
        "        rouge_f1_scores = [score['f1_score'].numpy() for score in rouge_scores]\n",
        "\n",
        "        avg_rouge_precision = np.mean(rouge_precisions)\n",
        "        avg_rouge_recall = np.mean(rouge_recalls)\n",
        "        avg_rouge_f1_score = np.mean(rouge_f1_scores)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} - ROUGE-L Precision: {avg_rouge_precision}, Recall: {avg_rouge_recall}, F1 Score: {avg_rouge_f1_score}\")\n",
        "        logs['ROUGE-L Precision'] = avg_rouge_precision\n",
        "        logs['ROUGE-L Recall'] = avg_rouge_recall\n",
        "        logs['ROUGE-L F1 Score'] = avg_rouge_f1_score\n",
        "\n",
        "\n",
        "class SaveLogsCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, log_file_path):\n",
        "        super().__init__()\n",
        "        self.log_file_path = log_file_path\n",
        "        self.logs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        logs['epoch'] = epoch + 1\n",
        "        # Convert float32 values to regular floats\n",
        "        logs = {k: (float(v) if isinstance(v, (np.float32, np.float64, tf.dtypes.DType)) else v) for k, v in logs.items()}\n",
        "        self.logs.append(logs)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        with open(self.log_file_path, 'w') as f:\n",
        "            json.dump(self.logs, f, indent=4)\n",
        "\n",
        "\n",
        "# Define the path where you want to save the logs\n",
        "log_file_name = 'test_training_logs.json'\n",
        "if '/' not in loss_model_path:\n",
        "    log_file_path = log_file_name\n",
        "else:\n",
        "    base_path = loss_model_path[:loss_model_path.rfind('/')]\n",
        "\n",
        "    log_file_path = os.path.join(base_path, log_file_name)\n",
        "\n",
        "print(log_file_path)\n",
        "# Create an instance of the callback\n",
        "save_logs_callback = SaveLogsCallback(log_file_path)\n",
        "\n",
        "\n",
        "history = LossHistory()\n",
        "vocab = vectorization.get_vocabulary()\n",
        "index_lookup = dict(zip(range(len(vocab)), vocab))\n",
        "bleu_callback = BleuScoreCallback(valid_dataset,10 ,vectorization, index_lookup, SEQ_LENGTH)\n",
        "\n",
        "# Example usage\n",
        "rouge_callback = RougeScoreCallback(\n",
        "    validation_dataset=valid_dataset,\n",
        "    dim_batch=10,\n",
        "    vectorization=vectorization,\n",
        "    index_lookup=index_lookup,\n",
        "    max_decoded_sentence_length=SEQ_LENGTH,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "perplexity_callback = PerplexityCallback(\n",
        "    validation_dataset=valid_dataset,\n",
        "    dim_batch=10,\n",
        "    vectorization=vectorization,\n",
        "    index_lookup=index_lookup,\n",
        "    max_decoded_sentence_length=SEQ_LENGTH\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "\n",
        "caption_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=valid_dataset,\n",
        "    callbacks=[early_stopping, cp_callback, history, bleu_callback, perplexity_callback, rouge_callback, save_logs_callback],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a7VdCRJNy6J"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from keras_nlp.metrics import Bleu\n",
        "# bleu = Bleu(max_order=3)\n",
        "# print(\"Calculating BLEU Score es:\" + str(bleu([\"references text\"], [\" references test\"]).numpy()))\n",
        "# bleu([\"references text\"], [\" references test\"]).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncB4oJA1Ny6K"
      },
      "outputs": [],
      "source": [
        "log_file_path = \"test_training_logs.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78uraeNrNy6K"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "# Load the JSON data from the log file\n",
        "with open(log_file_path, 'r') as f:\n",
        "    logs = json.load(f)\n",
        "\n",
        "# Extract metrics\n",
        "epochs = [log['epoch'] for log in logs]\n",
        "metrics = {key: [log[key] for log in logs if key in log] for key in logs[0].keys() if key != 'epoch'}\n",
        "\n",
        "# Determine the number of rows and columns for the grid\n",
        "num_metrics = len(metrics)\n",
        "num_cols = 3\n",
        "num_rows = math.ceil(num_metrics / num_cols)\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n",
        "\n",
        "# Flatten axes array for easy iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Plot each metric\n",
        "for idx, (metric, values) in enumerate(metrics.items()):\n",
        "    axes[idx].plot(epochs, values, label=metric)\n",
        "    axes[idx].set_xlabel('Epoch')\n",
        "    axes[idx].set_ylabel(metric)\n",
        "    axes[idx].set_title(f'{metric} over epochs')\n",
        "    axes[idx].legend()\n",
        "\n",
        "# Remove any empty subplots\n",
        "for idx in range(num_metrics, num_rows * num_cols):\n",
        "    fig.delaxes(axes[idx])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsUGiXCWCHZD"
      },
      "source": [
        "## Check sample predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIS_fh2ACHZD"
      },
      "outputs": [],
      "source": [
        "vocab = vectorization.get_vocabulary()\n",
        "index_lookup = dict(zip(range(len(vocab)), vocab))\n",
        "max_decoded_sentence_length = SEQ_LENGTH - 1\n",
        "valid_images = list(valid_data.keys())\n",
        "\n",
        "\n",
        "def generate_caption():\n",
        "    # Select a random image from the validation dataset\n",
        "    sample_img = np.random.choice(valid_images)\n",
        "\n",
        "    # Read the image from the disk\n",
        "    sample_img = decode_and_resize(\"coco/\" + \"train2014/\" + sample_img)\n",
        "    img = sample_img.numpy().clip(0, 255).astype(np.uint8)\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "    # Pass the image to the CNN\n",
        "    img = tf.expand_dims(sample_img, 0)\n",
        "    img = caption_model.cnn_model(img)\n",
        "\n",
        "    # Pass the image features to the Transformer encoder\n",
        "    encoded_img = caption_model.encoder(img, training=False)\n",
        "\n",
        "    # Generate the caption using the Transformer decoder\n",
        "    decoded_caption = \"<start> \"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_caption = vectorization([decoded_caption])[:, :-1]\n",
        "        mask = tf.math.not_equal(tokenized_caption, 0)\n",
        "        predictions = caption_model.decoder(\n",
        "            tokenized_caption, encoded_img, training=False, mask=mask\n",
        "        )\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = index_lookup[sampled_token_index]\n",
        "        if sampled_token == \"<end>\":\n",
        "            break\n",
        "        decoded_caption += \" \" + sampled_token\n",
        "\n",
        "    decoded_caption = decoded_caption.replace(\"<start> \", \"\")\n",
        "    decoded_caption = decoded_caption.replace(\" <end>\", \"\").strip()\n",
        "    print(\"Predicted Caption: \", decoded_caption)\n",
        "\n",
        "\n",
        "# Check predictions for a few samples\n",
        "generate_caption()\n",
        "generate_caption()\n",
        "generate_caption()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWXLGRjPCHZD"
      },
      "source": [
        "## End Notes\n",
        "\n",
        "We saw that the model starts to generate reasonable captions after a few epochs. To keep\n",
        "this example easily runnable, we have trained it with a few constraints, like a minimal\n",
        "number of attention heads. To improve the predictions, you can try changing these training\n",
        "settings and find a good model for your use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWJhFYu6Ny6L"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}