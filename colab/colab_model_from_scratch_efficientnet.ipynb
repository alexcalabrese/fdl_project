{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! wget http://images.cocodataset.org/zips/train2014.zip\n",
        "! wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
        "! sudo apt-get install unzip\n",
        "! unzip train2014.zip -d coco\n",
        "! unzip annotations_trainval2014.zip -d coco"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FydDSaeCHY9"
      },
      "source": [
        "# Image Captioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "class DatasetLoader:\n",
        "\n",
        "    def __init__(self, folder_name: str = \"coco\"):\n",
        "        self.data = self.load_data(folder_name)\n",
        "\n",
        "    def load_data(self, folder_name: str = \"coco\"):\n",
        "        print(\"Loading data..\")\n",
        "        # Set the data folder\n",
        "        data_folder = os.path.join(folder_name)\n",
        "\n",
        "        # Create the directory if it doesn't exist\n",
        "        if not os.path.exists(data_folder):\n",
        "            os.makedirs(data_folder)\n",
        "\n",
        "        # Path to the captions JSON file\n",
        "        filename = os.path.join(data_folder, \"annotations\", \"captions_train2014.json\")\n",
        "\n",
        "        # Read and parse the JSON file\n",
        "        with open(filename, \"r\") as json_file:\n",
        "            data = json.load(json_file)\n",
        "\n",
        "        print(\"Data loaded succesfully!\")\n",
        "\n",
        "        return data\n",
        "\n",
        "    def get_data_dict(self):\n",
        "        return self.data\n",
        "\n",
        "    def get_df_annotations(self):\n",
        "        return pd.DataFrame(self.data[\"annotations\"])\n",
        "\n",
        "    def get_df_images(self):\n",
        "        return pd.DataFrame(self.data[\"images\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLo0W6vdCHY_"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvIF03YJCHZA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.applications import efficientnet\n",
        "from keras.layers import TextVectorization\n",
        "\n",
        "keras.utils.set_random_seed(111)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4kll22_CHZA"
      },
      "source": [
        "## Download the dataset\n",
        "\n",
        "We will be using the Flickr8K dataset for this tutorial. This dataset comprises over\n",
        "8,000 images, that are each paired with five different captions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bo18px8lCHZB"
      },
      "outputs": [],
      "source": [
        "# Desired image dimensions\n",
        "IMAGE_SIZE = (299, 299)\n",
        "\n",
        "# Vocabulary size\n",
        "VOCAB_SIZE = 10000\n",
        "\n",
        "# Fixed length allowed for any sequence\n",
        "SEQ_LENGTH = 25\n",
        "\n",
        "# Dimension for the image embeddings and token embeddings\n",
        "EMBED_DIM = 512\n",
        "\n",
        "# Per-layer units in the feed-forward network\n",
        "FF_DIM = 512\n",
        "\n",
        "# Other training parameters\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 30\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsKZ446zCHZB"
      },
      "source": [
        "## Preparing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.xception import preprocess_input\n",
        "# from fdl_project.classes.dataset_loader import DatasetLoader\n",
        "\n",
        "# Load the dataset\n",
        "dataset = DatasetLoader().load_data(folder_name=os.path.join(os.getcwd(), \"coco\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Bm3LtvACHZB"
      },
      "outputs": [],
      "source": [
        "def load_captions_data(dataset):\n",
        "    text_annotations = []\n",
        "\n",
        "    # Create a mapping from image_id to file_name\n",
        "    image_id_to_file_name = {image[\"id\"]: image[\"file_name\"] for image in dataset[\"images\"]}\n",
        "\n",
        "    # Create a mapping from image_id to captions\n",
        "    image_id_to_captions = {}\n",
        "    for annotation in dataset[\"annotations\"]:\n",
        "        image_id = annotation[\"image_id\"]\n",
        "        caption = \"<start> \" + annotation[\"caption\"] + \" <end>\"\n",
        "\n",
        "        if image_id not in image_id_to_captions:\n",
        "            image_id_to_captions[image_id] = []\n",
        "\n",
        "        image_id_to_captions[image_id].append(caption)\n",
        "        text_annotations.append(caption)\n",
        "\n",
        "    # Create a combined dictionary with image file names and their corresponding captions\n",
        "    file_name_to_captions = {\n",
        "        image_id_to_file_name[image_id]: captions\n",
        "        for image_id, captions in image_id_to_captions.items()\n",
        "    }\n",
        "\n",
        "    return file_name_to_captions, text_annotations\n",
        "\n",
        "\n",
        "def train_val_split(caption_data, train_size=0.8, shuffle=True):\n",
        "    \"\"\"Split the captioning dataset into train and validation sets.\n",
        "\n",
        "    Args:\n",
        "        caption_data (dict): Dictionary containing the mapped caption data\n",
        "        train_size (float): Fraction of all the full dataset to use as training data\n",
        "        shuffle (bool): Whether to shuffle the dataset before splitting\n",
        "\n",
        "    Returns:\n",
        "        Traning and validation datasets as two separated dicts\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Get the list of all image names\n",
        "    all_images = list(caption_data.keys())\n",
        "\n",
        "    # 2. Shuffle if necessary\n",
        "    if shuffle:\n",
        "        np.random.shuffle(all_images)\n",
        "\n",
        "    # 3. Split into training and validation sets\n",
        "    train_size = int(len(caption_data) * train_size)\n",
        "\n",
        "    training_data = {\n",
        "        img_name: caption_data[img_name] for img_name in all_images[:train_size]\n",
        "    }\n",
        "    validation_data = {\n",
        "        img_name: caption_data[img_name] for img_name in all_images[train_size:]\n",
        "    }\n",
        "\n",
        "    # 4. Return the splits\n",
        "    return training_data, validation_data\n",
        "\n",
        "# Load the dataset\n",
        "captions_mapping, text_data = load_captions_data(dataset)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_data, valid_data = train_val_split(captions_mapping)\n",
        "print(\"Number of training samples: \", len(train_data))\n",
        "print(\"Number of validation samples: \", len(valid_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "captions_mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "727k9AQ_CHZC"
      },
      "source": [
        "## Vectorizing the text data\n",
        "\n",
        "We'll use the `TextVectorization` layer to vectorize the text data,\n",
        "that is to say, to turn the\n",
        "original strings into integer sequences where each integer represents the index of\n",
        "a word in a vocabulary. We will use a custom string standardization scheme\n",
        "(strip punctuation characters except `<` and `>`) and the default\n",
        "splitting scheme (split on whitespace)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OqgwC3YCHZC"
      },
      "outputs": [],
      "source": [
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "\n",
        "strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
        "strip_chars = strip_chars.replace(\"<\", \"\")\n",
        "strip_chars = strip_chars.replace(\">\", \"\")\n",
        "\n",
        "vectorization = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=SEQ_LENGTH,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "vectorization.adapt(text_data)\n",
        "\n",
        "# Data augmentation for image data\n",
        "image_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.2),\n",
        "        layers.RandomContrast(0.3),\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3KIwKe9CHZC"
      },
      "source": [
        "## Building a `tf.data.Dataset` pipeline for training\n",
        "\n",
        "We will generate pairs of images and corresponding captions using a `tf.data.Dataset` object.\n",
        "The pipeline consists of two steps:\n",
        "\n",
        "1. Read the image from the disk\n",
        "2. Tokenize all the five captions corresponding to the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# append coco/train2014/ to all the keys of the dictionary\n",
        "\n",
        "train_data = {\n",
        "    os.path.join(\"coco/train2014/\", key): value for key, value in train_data.items()\n",
        "}\n",
        "\n",
        "valid_data = {\n",
        "    os.path.join(\"coco/train2014/\", key): value for key, value in valid_data.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data[\"coco/train2014/COCO_train2014_000000254362.jpg\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check in train_data if there are more than 5 values\n",
        "# remove the 6th value and keep the first 5 values\n",
        "\n",
        "for key, value in train_data.items():\n",
        "    if len(value) > 5:\n",
        "        # print(key, len(value))\n",
        "        train_data[key] = value[:5]\n",
        "        \n",
        "for key, value in valid_data.items():\n",
        "    if len(value) > 5:\n",
        "        # print(key, len(value))\n",
        "        valid_data[key] = value[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajmZktjnCHZC"
      },
      "outputs": [],
      "source": [
        "def decode_and_resize(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SIZE)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    return img\n",
        "\n",
        "\n",
        "def process_input(img_path, captions):\n",
        "    print(\"PROCESS_INPUT: LEN DATASET\", len(dataset))\n",
        "    return decode_and_resize(img_path), vectorization(captions)\n",
        "\n",
        "\n",
        "def make_dataset(images, captions):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((images, tf.ragged.constant((captions))))\n",
        "    dataset = dataset.shuffle(BATCH_SIZE * 8)\n",
        "    dataset = dataset.map(process_input, num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Pass the list of images and the list of corresponding captions\n",
        "train_dataset = make_dataset(list(train_data.keys()), list(train_data.values()))\n",
        "\n",
        "valid_dataset = make_dataset(list(valid_data.keys()), list(valid_data.values()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LagTSBZGCHZC"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "Our image captioning architecture consists of three models:\n",
        "\n",
        "1. A CNN: used to extract the image features\n",
        "2. A TransformerEncoder: The extracted image features are then passed to a Transformer\n",
        "                    based encoder that generates a new representation of the inputs\n",
        "3. A TransformerDecoder: This model takes the encoder output and the text data\n",
        "                    (sequences) as inputs and tries to learn to generate the caption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJYaesH8CHZC"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import (\n",
        "    Conv2D,\n",
        "    MaxPooling2D,\n",
        "    Flatten,\n",
        "    Dense,\n",
        "    Input,\n",
        "    Embedding,\n",
        "    LSTM,\n",
        "    Add,\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "def get_cnn_model():\n",
        "    base_model = efficientnet.EfficientNetB0(\n",
        "        input_shape=(*IMAGE_SIZE, 3),\n",
        "        include_top=False,\n",
        "        weights=\"imagenet\",\n",
        "    )\n",
        "    # We freeze our feature extractor\n",
        "    base_model.trainable = False\n",
        "    base_model_out = base_model.output\n",
        "    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)\n",
        "    cnn_model = keras.models.Model(base_model.input, base_model_out)\n",
        "    return cnn_model\n",
        "\n",
        "\n",
        "class LSTMDecoder(Model):\n",
        "    def __init__(self, vocab_size: int, embed_dim: int, lstm_units: int):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embed_dim, mask_zero=True)\n",
        "        self.lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
        "        self.dense1 = Dense(256, activation=\"relu\")\n",
        "        self.dense2 = Dense(vocab_size, activation=\"softmax\")\n",
        " \n",
        "    def call(self, inputs, encoder_output, training=False, mask=None):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        lstm_output, state_h, state_c = self.lstm(\n",
        "            embeddings, initial_state=[encoder_output, encoder_output]\n",
        "        )\n",
        "        lstm_output = Add()([lstm_output, encoder_output])\n",
        "        lstm_output = self.dense1(lstm_output)\n",
        "        output = self.dense2(lstm_output)\n",
        "        return output\n",
        "\n",
        "class ImageCaptioningModel(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        cnn_model,\n",
        "        encoder,\n",
        "        decoder,\n",
        "        num_captions_per_image=5,\n",
        "        image_aug=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n",
        "        self.num_captions_per_image = num_captions_per_image\n",
        "        self.image_aug = image_aug\n",
        " \n",
        "    def calculate_loss(self, y_true, y_pred, mask):\n",
        "        loss = self.loss(y_true, y_pred)\n",
        "        mask = tf.cast(mask, dtype=loss.dtype)\n",
        "        loss *= mask\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        " \n",
        "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
        "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
        "        accuracy = tf.math.logical_and(mask, accuracy)\n",
        "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
        "        mask = tf.cast(mask, dtype=tf.float32)\n",
        "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
        " \n",
        "    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):\n",
        "        encoder_out = self.encoder(img_embed, training=training)\n",
        "        batch_seq_inp = batch_seq[:, :-1]\n",
        "        batch_seq_true = batch_seq[:, 1:]\n",
        "        mask = tf.math.not_equal(batch_seq_true, 0)\n",
        "        batch_seq_pred = self.decoder(\n",
        "            batch_seq_inp, encoder_out, training=training, mask=mask\n",
        "        )\n",
        "        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n",
        "        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n",
        "        return loss, acc\n",
        " \n",
        "    def train_step(self, batch_data):\n",
        "        batch_img, batch_seq = batch_data\n",
        "        batch_loss = 0\n",
        "        batch_acc = 0\n",
        " \n",
        "        if self.image_aug:\n",
        "            batch_img = self.image_aug(batch_img)\n",
        " \n",
        "        # 1. Get image embeddings\n",
        "        img_embed = self.cnn_model(batch_img)\n",
        " \n",
        "        # 2. Pass each of the five captions one by one to the decoder\n",
        "        # along with the encoder outputs and compute the loss as well as accuracy\n",
        "        # for each caption.\n",
        "        for i in range(self.num_captions_per_image):\n",
        "            with tf.GradientTape() as tape:\n",
        "                loss, acc = self._compute_caption_loss_and_acc(\n",
        "                    img_embed, batch_seq[:, i, :], training=True\n",
        "                )\n",
        " \n",
        "                # 3. Update loss and accuracy\n",
        "                batch_loss += loss\n",
        "                batch_acc += acc\n",
        " \n",
        "            # 4. Get the list of all the trainable weights\n",
        "            train_vars = (\n",
        "                self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "            )\n",
        " \n",
        "            # 5. Get the gradients\n",
        "            grads = tape.gradient(loss, train_vars)\n",
        " \n",
        "            # 6. Update the trainable weights\n",
        "            self.optimizer.apply_gradients(zip(grads, train_vars))\n",
        " \n",
        "        # 7. Update the trackers\n",
        "        batch_acc /= float(self.num_captions_per_image)\n",
        "        self.loss_tracker.update_state(batch_loss)\n",
        "        self.acc_tracker.update_state(batch_acc)\n",
        " \n",
        "        # 8. Return the loss and accuracy values\n",
        "        return {\n",
        "            \"loss\": self.loss_tracker.result(),\n",
        "            \"acc\": self.acc_tracker.result(),\n",
        "        }\n",
        " \n",
        "    def test_step(self, batch_data):\n",
        "        batch_img, batch_seq = batch_data\n",
        "        batch_loss = 0\n",
        "        batch_acc = 0\n",
        " \n",
        "        # 1. Get image embeddings\n",
        "        img_embed = self.cnn_model(batch_img)\n",
        " \n",
        "        # 2. Pass each of the five captions one by one to the decoder\n",
        "        # along with the encoder outputs and compute the loss as well as accuracy\n",
        "        # for each caption.\n",
        "        for i in range(self.num_captions_per_image):\n",
        "            loss, acc = self._compute_caption_loss_and_acc(\n",
        "                img_embed, batch_seq[:, i, :], training=False\n",
        "            )\n",
        " \n",
        "            # 3. Update batch loss and batch accuracy\n",
        "            batch_loss += loss\n",
        "            batch_acc += acc\n",
        " \n",
        "        batch_acc /= float(self.num_captions_per_image)\n",
        " \n",
        "        # 4. Update the trackers\n",
        "        self.loss_tracker.update_state(batch_loss)\n",
        "        self.acc_tracker.update_state(batch_acc)\n",
        " \n",
        "        # 5. Return the loss and accuracy values\n",
        "        return {\n",
        "            \"loss\": self.loss_tracker.result(),\n",
        "            \"acc\": self.acc_tracker.result(),\n",
        "        }\n",
        " \n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We need to list our metrics here so the `reset_states()` can be\n",
        "        # called automatically.\n",
        "        return [self.loss_tracker, self.acc_tracker]\n",
        "\n",
        "cnn_model = get_cnn_model(input_shape=(*IMAGE_SIZE, 3))\n",
        "encoder = Dense(256, activation=\"relu\")  # Simple encoder for compatibility\n",
        "decoder = LSTMDecoder(vocab_size=VOCAB_SIZE, embed_dim=EMBED_DIM, lstm_units=256)\n",
        "image_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.2),\n",
        "        layers.RandomContrast(0.3),\n",
        "    ]\n",
        ")\n",
        "\n",
        "caption_model = ImageCaptioningModel(\n",
        "    cnn_model=cnn_model,\n",
        "    encoder=encoder,\n",
        "    decoder=decoder,\n",
        "    image_aug=image_augmentation,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rxhYJ-XCHZD"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    import datetime\n",
        "    drive.mount('/content/gdrive')\n",
        "\n",
        "    # Create a directory in Google Drive to store models\n",
        "    !mkdir /content/gdrive/My\\ Drive/Colab_Models\n",
        "    loss_model_path = \"/content/gdrive/My Drive/Colab_Models/loss_model_from_scratch_efficientnet_\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".txt\"\n",
        "    checkpoint_path = \"/content/gdrive/My Drive/Colab_Models/cp_model_from_scratch_efficientnet_\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".h5\"\n",
        "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "    open(loss_model_path, 'w').close()\n",
        "except:\n",
        "    loss_model_path = \"loss_model_from_scratch_LSTM_CNN.txt\"\n",
        "    checkpoint_path = \"cp_model_from_scratch_LSTM_CNN.weights.h5\"\n",
        "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "    open(loss_model_path, 'w').close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lO59QfwcCHZD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the loss function\n",
        "cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=False,\n",
        "    reduction='none',\n",
        ")\n",
        "\n",
        "# EarlyStopping criteria\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "# Learning Rate Scheduler for the optimizer\n",
        "class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, post_warmup_learning_rate, warmup_steps):\n",
        "        super().__init__()\n",
        "        self.post_warmup_learning_rate = post_warmup_learning_rate\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        global_step = tf.cast(step, tf.float32)\n",
        "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
        "        warmup_progress = global_step / warmup_steps\n",
        "        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n",
        "        return tf.cond(\n",
        "            global_step < warmup_steps,\n",
        "            lambda: warmup_learning_rate,\n",
        "            lambda: self.post_warmup_learning_rate,\n",
        "        )\n",
        "\n",
        "\n",
        "# Create a learning rate schedule\n",
        "num_train_steps = len(train_dataset) * EPOCHS\n",
        "num_warmup_steps = num_train_steps // 15\n",
        "lr_schedule = LRSchedule(post_warmup_learning_rate=1e-4, warmup_steps=num_warmup_steps)\n",
        "\n",
        "# Compile the model\n",
        "caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import _pickle as pickle\n",
        "\n",
        "class LossHistory(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.history = {'loss':[],'val_loss':[]}\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.history['loss'].append(logs.get('loss'))\n",
        "        with open(loss_model_path, \"r+b\") as myfile:\n",
        "            myfile.write(pickle.dumps(self.history))\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.history['val_loss'].append(logs.get('val_loss'))\n",
        "\n",
        "history = LossHistory()\n",
        "\n",
        "# Fit the model\n",
        "caption_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=valid_dataset,\n",
        "    callbacks=[early_stopping, cp_callback, history],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsUGiXCWCHZD"
      },
      "source": [
        "## Check sample predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIS_fh2ACHZD"
      },
      "outputs": [],
      "source": [
        "vocab = vectorization.get_vocabulary()\n",
        "index_lookup = dict(zip(range(len(vocab)), vocab))\n",
        "max_decoded_sentence_length = SEQ_LENGTH - 1\n",
        "valid_images = list(valid_data.keys())\n",
        "\n",
        "\n",
        "def generate_caption():\n",
        "    # Select a random image from the validation dataset\n",
        "    sample_img = np.random.choice(valid_images)\n",
        "\n",
        "    # Read the image from the disk\n",
        "    sample_img = decode_and_resize(\"coco/\" + \"train2014/\" + sample_img)\n",
        "    img = sample_img.numpy().clip(0, 255).astype(np.uint8)\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "    # Pass the image to the CNN\n",
        "    img = tf.expand_dims(sample_img, 0)\n",
        "    img = caption_model.cnn_model(img)\n",
        "\n",
        "    # Pass the image features to the Transformer encoder\n",
        "    encoded_img = caption_model.encoder(img, training=False)\n",
        "\n",
        "    # Generate the caption using the Transformer decoder\n",
        "    decoded_caption = \"<start> \"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_caption = vectorization([decoded_caption])[:, :-1]\n",
        "        mask = tf.math.not_equal(tokenized_caption, 0)\n",
        "        predictions = caption_model.decoder(\n",
        "            tokenized_caption, encoded_img, training=False, mask=mask\n",
        "        )\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = index_lookup[sampled_token_index]\n",
        "        if sampled_token == \"<end>\":\n",
        "            break\n",
        "        decoded_caption += \" \" + sampled_token\n",
        "\n",
        "    decoded_caption = decoded_caption.replace(\"<start> \", \"\")\n",
        "    decoded_caption = decoded_caption.replace(\" <end>\", \"\").strip()\n",
        "    print(\"Predicted Caption: \", decoded_caption)\n",
        "\n",
        "\n",
        "# Check predictions for a few samples\n",
        "generate_caption()\n",
        "generate_caption()\n",
        "generate_caption()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWXLGRjPCHZD"
      },
      "source": [
        "## End Notes\n",
        "\n",
        "We saw that the model starts to generate reasonable captions after a few epochs. To keep\n",
        "this example easily runnable, we have trained it with a few constraints, like a minimal\n",
        "number of attention heads. To improve the predictions, you can try changing these training\n",
        "settings and find a good model for your use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "image_captioning",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
